# SCREAM to HEALPix Processing Configuration
# Optimized for NERSC Perlmutter (128-core, 512GB RAM compute nodes)

# Data paths
input_base_dir: "/global/cfs/cdirs/e3sm/beydoun/ne1024pg2_ne1024pg2.F2010-SCREAMv1.10022025.CESS2.P0K.n2048.2/run/"
output_base_dir: "/pscratch/sd/w/wcmca1/hackathon/healpix/scream_cess2/"
output_basename: "SCREAMv1_ne120_AVERAGE"

# Weights file configuration (REQUIRED)
# Specify the path where remapping weights will be stored
# - If file exists: Weights are loaded for fast remapping
# - If file doesn't exist: Weights are computed once and saved for reuse
# - Subsequent runs reuse cached weights for much faster processing
# Example: "/path/to/weights/scream_ne120pg2_to_healpix_z8_weights.nc"
weights_file: "/pscratch/sd/w/wcmca1/hackathon/healpix/scream_cess2/weights/scream_ne120pg2_to_healpix_z8_weights.nc"

# File search pattern configuration
# These parameters control how files are discovered and filtered by date
# Filename format: 1ma_ne120pg2_dyamond3.AVERAGE.nmonths_x1.YYYY-MM-DD-xxxxx.nc (e.g., 1ma_ne120pg2_dyamond3.AVERAGE.nmonths_x1.2019-08-01-00000.nc)
date_pattern: "\\.(\\d{4}-\\d{2}-\\d{2})-"   # Regex to extract YYYY-MM-DD
date_format: "%Y-%m-%d"        # strptime format: YYYY=year, MM=month, DD=day
file_glob: "1ma_ne120pg2_dyamond3.AVERAGE.nmonths_x1.*.nc"         # Glob pattern to match files (merg_*.nc)
use_year_subdirs: False         # Files organized in yearly subdirectories (YYYY/)

# Processing parameters
default_zoom: 8
time_chunk_size: 1  # 1 month worth of 1-month data (balance memory vs parallelism)
time_average: null  # Options: null (no averaging), "1h" (1-hour), "3h", "6h", "1d" (daily), etc.
original_time_suffix: "1M"  # Time resolution of source data for output filename (e.g., '1H', '3H', '1D')
convert_time: True   # Convert cftime.DatetimeJulian to standard datetime64 for pandas compatibility

# Variables to remap and rename
remap_variables:
  T_mid: ta
  qv: hus
  RelativeHumidity: hur
  z_mid: z
  p_mid: p
  U: ua
  V: va
  omega: omega
  cldfrac_tot_for_analysis: cldfrac_tot_for_analysis
  ps: ps
  T_2m: tas
  qv_2m: huss
  RelativeHumidity_at_2m_above_surface: hurs
  wind_speed_10m: sfcWind
  SeaLevelPressure: psl
  omega_at_500hPa: omega500
  z_mid_at_500hPa: z500
  MeridionalVapFlux: vivt
  ZonalVapFlux: uivt
  surface_upward_latent_heat_flux: hflsd
  surf_sens_flux: hfssd
  VapWaterPath: prw
  LiqWaterPath: lwp
  IceWaterPath: iwp
  RainWaterPath: rwp
  ShortwaveCloudForcing: ShortwaveCloudForcing
  LongwaveCloudForcing: LongwaveCloudForcing
  landfrac: landfrac

# Variables to pass through without remapping (e.g., 1D coordinates)
# These will be copied directly to the output dataset
passthrough_variables:
  - p_levs

# Only process variables with these dimensions
# This allows both 2D (time, ncol) and 3D (time, ncol, lev) variables
required_dimensions:
  - ["time", "ncol"]
  - ["time", "ncol", "lev"]

# Variable filtering
skip_variables:
  - "*band"
  - "*_bounds"
  - "time_bnds"
  - "areas"
  - "hyai"
  - "hyam"
  - "hybi"
  - "hybm"
  # Note: lev and ilev are kept for 3D variables

# Grid type configuration
# Specifies the type of input grid for proper weight generation
# Options: 'auto' (detect), 'latlon_1d' (regular), 'latlon_2d' (curvilinear), 'unstructured' (E3SM/SCREAM)
grid_type: 'unstructured'  # SCREAM uses unstructured grid with ncol dimension

# Spatial dimension configuration
# For unstructured grids (E3SM/SCREAM), only x_dimname is needed
x_dimname: ncol      # Single spatial dimension for unstructured grid

# Coordinate variable names (if different from dimension names) 
x_coordname: lon      # Name of longitude coordinate variable
y_coordname: lat      # Name of latitude coordinate variable

# Spatial chunking specification
spatial_dimensions:
  ncol: -1   # -1 means no chunking (keep full dimension)

# Examples for different grid types:
#
# Standard IMERG/IR_IMERG (lat/lon grid):
#   x_dimname: lon
#   y_dimname: lat
#   spatial_dimensions:
#     lon: -1
#     lat: -1
#
# WRF output (custom dimension and coordinate names):
#   x_dimname: west_east
#   y_dimname: south_north
#   x_coordname: XLONG        # WRF uses XLONG for longitude coordinate
#   y_coordname: XLAT         # WRF uses XLAT for latitude coordinate
#   spatial_dimensions:
#     west_east: -1
#     south_north: -1
#
# E3SM/SCREAM (unstructured grid):
#   x_dimname: ncol           # Only x_dimname needed for unstructured grids
#   spatial_dimensions:
#     ncol: -1
#
# CMIP6 (varies by model):
#   x_dimname: i              # or 'lon', 'x', depending on model
#   y_dimname: j              # or 'lat', 'y', depending on model
#   x_coordname: longitude    # or 'lon', 'LONGITUDE'
#   y_coordname: latitude     # or 'lat', 'LATITUDE'
#   spatial_dimensions:
#     i: -1
#     j: -1

# Time dimension configuration (OPTIONAL)
# Name of the time/concatenation dimension (default: 'time')
# Only specify if your dataset uses a different name
# concat_dim: "time"
#
# Examples for different datasets:
#   Most datasets: "time" (default)
#   WRF: "Time" or "Times"
#   Some CMIP6: "time_counter"

# Note: spatial_chunk_size is automatically computed based on zoom level using chunk_tools.compute_chunksize()
force_recompute: False    # Force recompute weight file even if it exists

# Dask configuration - SIMPLIFIED for I/O-intensive zarr writes
# Hardware: 2x AMD EPYC 7763 (128 cores total), 512 GB RAM
# Strategy: Fewer workers with more memory to avoid pause/deadlock issues
dask:
  n_workers: 16               # Fewer workers = less memory contention during I/O
  threads_per_worker: 1      # Moderate threading for I/O operations
  # memory_limit: "30GB"        # Auto-calculated: 80% of 512GB / 4 workers â‰ˆ 102GB per worker
  
  # Note: memory_limit commented out to use auto-calculation from system memory
  # This prevents worker pausing at 80% that causes zarr write deadlocks
  
# Compression settings for Zarr
compression:
  compressor: "zstd"
  compressor_level: 3
  dtype: "float32"
