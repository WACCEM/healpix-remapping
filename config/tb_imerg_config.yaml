# IMERG to HEALPix Processing Configuration
# Optimized for NERSC Perlmutter (128-core, 512GB RAM compute nodes)

# Data paths
input_base_dir: "/pscratch/sd/w/wcmca1/GPM/IR_IMERG_Combined_V07B/"
output_base_dir: "/pscratch/sd/w/wcmca1/GPM/healpix/test/"
output_basename: "IR_IMERG_V7"
weights_dir: "/pscratch/sd/w/wcmca1/GPM/weights/"

# File search pattern configuration
# These parameters control how files are discovered and filtered by date
# Filename format: merg_YYYYMMDDhh_10km-pixel.nc (e.g., merg_2020010100_10km-pixel.nc)
date_pattern: "_(\\d{10})_"    # Regex to extract YYYYMMDDhh (10 digits between underscores)
date_format: "%Y%m%d%H"        # strptime format: YYYY=year, MM=month, DD=day, HH=hour
file_glob: "merg_*.nc"         # Glob pattern to match files (merg_*.nc)
use_year_subdirs: true         # Files organized in yearly subdirectories (YYYY/)

# Processing parameters  
default_zoom: 9
time_chunk_size: 24  # 1 day worth of 1-hour data (balance memory vs parallelism)
time_average: "1h"  # Options: null (no averaging), "1h" (1-hour), "3h", "6h", "1d" (daily), etc.
original_time_suffix: "30MIN"  # Time resolution of source data for output filename (e.g., '30MIN', '1H', '1D')
convert_time: True   # Convert cftime.DatetimeJulian to standard datetime64 for pandas compatibility

# Only process variables with these dimensions
required_dimensions:
  - ["time", "lat", "lon"]
  - ["time", "lon", "lat"]

# Variable filtering
skip_variables:
  - "*_bnds"
  - "*_bounds"
  - "time_bnds"
  - "lat_bnds" 
  - "lon_bnds"

# Spatial dimension configuration
# IMPORTANT: For 2D grids, explicitly specify both x_dimname and y_dimname to avoid ambiguity
# These define which dimensions represent the spatial axes in your data
x_dimname: lon       # Name of x spatial dimension (longitude)
y_dimname: lat       # Name of y spatial dimension (latitude)

# Coordinate variable names (if different from dimension names) 
x_coordname: lon      # Name of longitude coordinate variable
y_coordname: lat      # Name of latitude coordinate variable

# Spatial chunking specification
# Use -1 for no chunking (keep full dimension in memory - recommended for remapping)
spatial_dimensions:
  lon: -1   # Full longitude dimension in single chunk
  lat: -1   # Full latitude dimension in single chunk

# Coordinate variable names (if different from dimension names)
# Uncomment and modify if your dataset uses different coordinate variable names
# lon_name: lon          # Name of longitude coordinate variable
# lat_name: lat          # Name of latitude coordinate variable

# Examples for different grid types:
# 
# Standard IMERG (lat/lon grid):
#   x_dimname: lon
#   y_dimname: lat
#   spatial_dimensions:
#     lon: -1
#     lat: -1
#
# WRF output (custom dimension names):
#   x_dimname: west_east
#   y_dimname: south_north
#   x_coordname: XLONG        # WRF uses XLONG for longitude coordinate
#   y_coordname: XLAT         # WRF uses XLAT for latitude coordinate
#   spatial_dimensions:
#     west_east: -1
#     south_north: -1
#
# E3SM/SCREAM (unstructured grid):
#   x_dimname: ncol        # Only x_dimname needed for unstructured grids
#   spatial_dimensions:
#     ncol: -1
#
# CMIP6 (varies by model):
#   x_dimname: i           # or 'lon', 'x', depending on model
#   y_dimname: j           # or 'lat', 'y', depending on model
#   x_coordname: longitude    # or 'lon', 'LONGITUDE'
#   y_coordname: latitude     # or 'lat', 'LATITUDE'
#   spatial_dimensions:
#     i: -1
#     j: -1

# Time dimension configuration (OPTIONAL)
# Name of the time/concatenation dimension (default: 'time')
# Only specify if your dataset uses a different name
# concat_dim: "time"
#
# Examples for different datasets:
#   Most datasets: "time" (default)
#   WRF: "Time" or "Times"
#   Some CMIP6: "time_counter"

# Note: spatial_chunk_size is automatically computed based on zoom level using chunk_tools.compute_chunksize()
force_recompute: False    # Force recompute weight file even if it exists

# Dask configuration - SIMPLIFIED for I/O-intensive zarr writes
# Hardware: 2x AMD EPYC 7763 (128 cores total), 512 GB RAM
# Strategy: Fewer workers with more memory to avoid pause/deadlock issues
dask:
  n_workers: 16               # Fewer workers = less memory contention during I/O
  threads_per_worker: 1      # Moderate threading for I/O operations
  # memory_limit: "30GB"        # Auto-calculated: 80% of 512GB / 4 workers â‰ˆ 102GB per worker
  
  # Note: memory_limit commented out to use auto-calculation from system memory
  # This prevents worker pausing at 80% that causes zarr write deadlocks
  
# Compression settings for Zarr
compression:
  compressor: "zstd"
  compressor_level: 3
  dtype: "float32"